{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "import pandas as pd\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/media/akib/New Volume1/project/fer2013/fer2013.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion    0\n",
       "pixels     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df[[\"emotion\", \"pixels\"]][df[\"Usage\"] == \"Training\"]\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[70.0, 80.0, 82.0, 72.0, 58.0, 58.0, 60.0, 63....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[151.0, 150.0, 147.0, 155.0, 148.0, 133.0, 111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[231.0, 212.0, 156.0, 164.0, 174.0, 138.0, 161...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[24.0, 32.0, 36.0, 30.0, 32.0, 23.0, 19.0, 20....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>[4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels\n",
       "0        0  [70.0, 80.0, 82.0, 72.0, 58.0, 58.0, 60.0, 63....\n",
       "1        0  [151.0, 150.0, 147.0, 155.0, 148.0, 133.0, 111...\n",
       "2        2  [231.0, 212.0, 156.0, 164.0, 174.0, 138.0, 161...\n",
       "3        4  [24.0, 32.0, 36.0, 30.0, 32.0, 23.0, 19.0, 20....\n",
       "4        6  [4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['pixels'] = train['pixels'].apply(lambda im: np.fromstring(im, sep=' '))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.vstack(train['pixels'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 70.,  80.,  82., ..., 106., 109.,  82.],\n",
       "       [151., 150., 147., ..., 193., 183., 184.],\n",
       "       [231., 212., 156., ...,  88., 110., 152.],\n",
       "       ...,\n",
       "       [ 74.,  81.,  87., ..., 188., 187., 187.],\n",
       "       [222., 227., 203., ..., 136., 136., 134.],\n",
       "       [195., 199., 205., ...,   6.,  15.,  38.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28709, 2304), (28709,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(train[\"emotion\"])\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_test_df = df[[\"emotion\", \"pixels\"]][df[\"Usage\"]==\"PublicTest\"]\n",
    "private_test_df= df[[\"emotion\", \"pixels\"]][df[\"Usage\"] == \"PrivateTest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_test_df[\"pixels\"] = public_test_df[\"pixels\"].apply(lambda im: np.fromstring(im, sep=' '))\n",
    "x_test = np.vstack(public_test_df[\"pixels\"].values)\n",
    "y_test = np.array(public_test_df[\"emotion\"])\n",
    "private_test_df[\"pixels\"] = private_test_df[\"pixels\"].apply(lambda im: np.fromstring(im, sep=' '))\n",
    "x_test_private = np.vstack(private_test_df[\"pixels\"].values)\n",
    "y_test_private = np.array(private_test_df[\"emotion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28709, 48, 48, 1), (3589, 48, 48, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.reshape(-1, 48, 48, 1)\n",
    "x_test = x_test.reshape(-1, 48, 48, 1)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.dstack([x_train]*3)\n",
    "# x_test = np.dstack([x_test]*3)\n",
    "# x_train = x_train.reshape(-1, 48,48,3)\n",
    "# x_test= x_test.reshape (-1,48,48,3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.applications.vgg16 import preprocess_input\n",
    "# # prepare the image for the VGG model\n",
    "# image = preprocess_input(x_test_private[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train.shape\n",
    "# model_vgg16_conv = VGG16(weights='imagenet', include_top=False,input_shape=(48,48,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 46, 46, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 46, 46, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 46, 46, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 23, 23, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 23, 23, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 11, 11, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 11, 11, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 11, 11, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 5, 5, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_vgg16_conv = VGG16(weights='imagenet', include_top=False,input_shape=(46,46,3))\n",
    "model_vgg16_conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<keras.engine.input_layer.InputLayer object at 0x7f014c8a5690>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x7f01011e0a90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x7f01011e0d10>, False)\n",
      "(<keras.layers.pooling.MaxPooling2D object at 0x7f01497dc810>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x7f01011e49d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x7f0100dae790>, False)\n",
      "(<keras.layers.pooling.MaxPooling2D object at 0x7f0100daef90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x7f0100d40ad0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x7f0100d6bc90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x7f0100cfc490>, False)\n",
      "(<keras.layers.pooling.MaxPooling2D object at 0x7f0100d13650>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x7f0100d26790>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x7f0100cd1f50>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x7f0100ce1110>, False)\n",
      "(<keras.layers.pooling.MaxPooling2D object at 0x7f0100cf7250>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x7f0100c8abd0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x7f0100c9b850>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x7f0100cb37d0>, False)\n",
      "(<keras.layers.pooling.MaxPooling2D object at 0x7f0100c48950>, False)\n"
     ]
    }
   ],
   "source": [
    "# model_vgg16_conv.add(model.add(Conv2D(3,(3,3),input_shape = (48,48,1),activation = 'relu')))\n",
    "for layer in model_vgg16_conv.layers[:]:\n",
    "    layer.trainable = False\n",
    "for layer in  model_vgg16_conv.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 46, 46, 3)         30        \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                (None, 1, 1, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 14,879,845\n",
      "Trainable params: 165,157\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.layers.convolutional import Conv2D\n",
    "model = models.Sequential()\n",
    "model.add(Conv2D(3,(3,3),input_shape = (48,48,1),activation = 'relu'))\n",
    "model.add(model_vgg16_conv)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256,activation = 'relu',kernel_initializer = 'he_normal'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(128,activation='relu',kernel_initializer = 'he_normal'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(7,activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_datagen = ImageDataGenerator(\n",
    "#       rescale=1./255,\n",
    "#       rotation_range=20,\n",
    "#       width_shift_range=0.2,\n",
    "#       height_shift_range=0.2,\n",
    "#       horizontal_flip=True,\n",
    "#       fill_mode='nearest')\n",
    " \n",
    "# validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "# # Change the batchsize according to your system RAM\n",
    "# train_batchsize = 100\n",
    "# val_batchsize = 10\n",
    "# train_dir = \n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#         train_dir,\n",
    "#         target_size=(image_size, image_size),\n",
    "#         batch_size=train_batchsize,\n",
    "#         class_mode='categorical')\n",
    " \n",
    "# validation_generator = validation_datagen.flow_from_directory(\n",
    "#         validation_dir,\n",
    "#         target_size=(image_size, image_size),\n",
    "#         batch_size=val_batchsize,\n",
    "#         class_mode='categorical',\n",
    "#         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_model = Model(input = input1,output = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28709, 7), (3589, 7))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "# run model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 9.06 µs\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "type(model)\n",
    "%time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28709 samples, validate on 3589 samples\n",
      "Epoch 1/20\n",
      " - 1523s - loss: 1.6047 - acc: 0.3718 - val_loss: 1.5959 - val_acc: 0.3759\n",
      "Epoch 2/20\n",
      " - 1490s - loss: 1.5922 - acc: 0.3771 - val_loss: 1.5982 - val_acc: 0.3711\n",
      "Epoch 3/20\n",
      " - 1491s - loss: 1.5845 - acc: 0.3806 - val_loss: 1.6018 - val_acc: 0.3795\n",
      "Epoch 4/20\n",
      " - 1489s - loss: 1.5671 - acc: 0.3888 - val_loss: 1.6073 - val_acc: 0.3709\n",
      "Epoch 5/20\n",
      " - 1490s - loss: 1.5584 - acc: 0.3930 - val_loss: 1.5821 - val_acc: 0.3926\n",
      "Epoch 6/20\n",
      " - 1490s - loss: 1.5470 - acc: 0.3988 - val_loss: 1.5800 - val_acc: 0.3901\n",
      "Epoch 7/20\n",
      " - 1490s - loss: 1.5340 - acc: 0.4057 - val_loss: 1.5835 - val_acc: 0.3853\n",
      "Epoch 8/20\n",
      " - 1490s - loss: 1.5179 - acc: 0.4136 - val_loss: 1.5825 - val_acc: 0.3920\n",
      "Epoch 9/20\n",
      " - 1490s - loss: 1.5110 - acc: 0.4172 - val_loss: 1.5796 - val_acc: 0.3904\n",
      "Epoch 10/20\n",
      " - 1491s - loss: 1.4966 - acc: 0.4202 - val_loss: 1.5761 - val_acc: 0.3892\n",
      "Epoch 11/20\n",
      " - 1490s - loss: 1.4855 - acc: 0.4252 - val_loss: 1.5779 - val_acc: 0.3926\n",
      "Epoch 12/20\n",
      " - 1490s - loss: 1.4728 - acc: 0.4264 - val_loss: 1.5808 - val_acc: 0.3887\n",
      "Epoch 13/20\n",
      " - 1490s - loss: 1.4606 - acc: 0.4386 - val_loss: 1.5721 - val_acc: 0.3920\n",
      "Epoch 14/20\n",
      " - 1489s - loss: 1.4500 - acc: 0.4411 - val_loss: 1.5693 - val_acc: 0.4029\n",
      "Epoch 15/20\n",
      " - 1490s - loss: 1.4432 - acc: 0.4441 - val_loss: 1.5776 - val_acc: 0.4037\n",
      "Epoch 16/20\n",
      " - 1490s - loss: 1.4357 - acc: 0.4459 - val_loss: 1.5760 - val_acc: 0.3970\n",
      "Epoch 17/20\n",
      " - 1490s - loss: 1.4222 - acc: 0.4553 - val_loss: 1.5755 - val_acc: 0.3982\n",
      "Epoch 18/20\n",
      " - 1490s - loss: 1.4167 - acc: 0.4555 - val_loss: 1.5808 - val_acc: 0.3970\n",
      "Epoch 19/20\n",
      " - 1489s - loss: 1.4007 - acc: 0.4620 - val_loss: 1.5775 - val_acc: 0.4037\n",
      "Epoch 20/20\n",
      " - 1490s - loss: 1.3871 - acc: 0.4674 - val_loss: 1.5847 - val_acc: 0.4043\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 14.5 ms\n"
     ]
    }
   ],
   "source": [
    "model = load_model('/home/akib/weights.h5')\n",
    "hist = model.fit(x_train, y_train, epochs=20,\n",
    "                 shuffle=True,\n",
    "                 batch_size=100, validation_data=(x_test, y_test),\n",
    "                  verbose=2)\n",
    "model.save('weights.h5')\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.applications.vgg16 import preprocess_input\n",
    "# # prepare the image for the VGG model\n",
    "# image = preprocess_input(x_test_private[1])\n",
    "# test = x_test_private[5]\n",
    "# test = np.dstack([test]*3)\n",
    "# test = test.reshape(-1,48,48,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_private = np_utils.to_categorical(y_test_private)\n",
    "# x_test_private = np.dstack([x_test_private]*3)\n",
    "# x_test_private = x_test_private.reshape(-1,48,48,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_private = x_test_private.reshape(-1,48,48,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.43159654416822\n"
     ]
    }
   ],
   "source": [
    "scores=model.evaluate(x_test_private,y_test_private,verbose=0)\n",
    "print(100-scores[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5453918570760035"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40568403455831775"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral\n",
    "dic = {0:'Angry',1:'Disgust',2:'Fear',3:'Happy',4:'Sad',5:'Surprise',6:'Neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_private[0] = x_test_private[0].reshape(1,48,48,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_private[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_private[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df1 = pd.read_csv(\"/media/akib/New Volume1/project/fer2013/fer2013.csv\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test_df= df1[[\"emotion\", \"pixels\"]][df1[\"Usage\"] == \"PrivateTest\"]\n",
    "private_test_df[\"pixels\"] = private_test_df[\"pixels\"].apply(lambda im: np.fromstring(im, sep=' '))\n",
    "x_test_private = np.vstack(private_test_df[\"pixels\"].values)\n",
    "y_test_private = np.array(private_test_df[\"emotion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_private = x_test_private.reshape(-1,48,48,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_private1 = df1[[\"emotion\", \"pixels\"]][df1[\"Usage\"] == \"PrivateTest\"]\n",
    "# train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_private1[\"pixels\"] = test_private1[\"pixels\"].apply(lambda im: np.fromstring(im, sep=' '))\n",
    "xtest = np.vstack(test_private1[\"pixels\"].values)\n",
    "ytest = np.array(test_private1[\"emotion\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {0:'Angry',1:'Disgust',2:'Fear',3:'Happy',4:'Sad',5:'Surprise',6:'Neutral'}\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('/home/akib/weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc931677b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numb=115\n",
    "tod = xtest[numb].reshape(48,48)\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(tod, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Happy'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic[ytest[numb]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12.49268427491188, 'Angry')\n",
      "(1.5887923538684845, 'Disgust')\n",
      "(16.826993227005005, 'Fear')\n",
      "(10.104473680257797, 'Happy')\n",
      "(26.8909752368927, 'Sad')\n",
      "(3.587374836206436, 'Surprise')\n",
      "(28.508716821670532, 'Neutral')\n"
     ]
    }
   ],
   "source": [
    "x_test_privat = np.expand_dims(x_test_private[numb], axis=0)\n",
    "\n",
    "\n",
    "g=model.predict(x_test_privat)[0]\n",
    "\n",
    "for x in g:\n",
    "    print(x*100,dic[np.where(g==x)[0][0]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    8989\n",
       "6    6198\n",
       "4    6077\n",
       "2    5121\n",
       "0    4953\n",
       "5    4002\n",
       "1     547\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Angry',\n",
       " 1: 'Disgust',\n",
       " 2: 'Fear',\n",
       " 3: 'Happy',\n",
       " 4: 'Sad',\n",
       " 5: 'Surprise',\n",
       " 6: 'Neutral'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
